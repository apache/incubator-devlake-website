"use strict";(self.webpackChunkwww=self.webpackChunkwww||[]).push([[81045],{12190:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"setting-up-devlake-in-aws-ecs-cluster-using-terraform","metadata":{"permalink":"/zh/blog/setting-up-devlake-in-aws-ecs-cluster-using-terraform","editUrl":"https://github.com/apache/incubator-devlake-website/edit/main/blog/2025-04-03-setting-up-devlake-in-aws-ecs-cluster-using-terraform/index.md","source":"@site/blog/2025-04-03-setting-up-devlake-in-aws-ecs-cluster-using-terraform/index.md","title":"Setting up Devlake in AWS ECS cluster using Terraform","description":"This guide provides an alternative to Devlake\'s Kubernetes setup and will walk you through setting up DevLake in an AWS ECS (Elastic Container Service) cluster using Terraform, providing you with a scalable and maintainable infrastructure as code solution that is also cost effective.","date":"2025-04-03T00:00:00.000Z","formattedDate":"2025\u5e744\u67083\u65e5","tags":[{"label":"DevLake","permalink":"/zh/blog/tags/dev-lake"},{"label":"AWS","permalink":"/zh/blog/tags/aws"},{"label":"ECS","permalink":"/zh/blog/tags/ecs"},{"label":"Terraform","permalink":"/zh/blog/tags/terraform"}],"readingTime":9.23,"hasTruncateMarker":true,"authors":[{"name":"TinaZhouHui","title":"Devlake Enthusiast","url":"https://github.com/tinazhouhui","imageURL":"https://avatars.githubusercontent.com/u/63497846?v=4","key":"TinaZhouHui"}],"frontMatter":{"slug":"setting-up-devlake-in-aws-ecs-cluster-using-terraform","title":"Setting up Devlake in AWS ECS cluster using Terraform","authors":"TinaZhouHui","tags":["DevLake","AWS","ECS","Terraform"]},"nextItem":{"title":"Quick Start Guide: Setup Your First Engineering Metrics Dashboard in 5 Minutes","permalink":"/zh/blog/Quick-Start-Guide-Setup-Your-First-Engineering-Metrics-Dashboard-in-5-Minutes"}},"content":"This guide provides an alternative to Devlake\'s Kubernetes setup and will walk you through setting up DevLake in an AWS ECS (Elastic Container Service) cluster using Terraform, providing you with a scalable and maintainable infrastructure as code solution that is also cost effective.\\n\\nBasic knowledge of AWS ECS, Terraform, and your network where your cluster will be deployed is necessary to follow this guide effectively.\\n\\n\x3c!--truncate--\x3e\\n\\n# Setting up terraform structure\\nTo configure devlake, we will use a terraform module and pass the necessary variables. The final structure of the module would be something like this:\\n\\n```\\nterraform-root/\\n\u251c\u2500\u2500 main.tf\\n\u251c\u2500\u2500 ...\\n\u251c\u2500\u2500 modules/\\n\u2502   \u251c\u2500\u2500 devlake/\\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf\\n\u2502   \u2502   \u251c\u2500\u2500 rds.tf\\n\u2502   \u2502   \u251c\u2500\u2500 alb.tf\\n\u2502   \u2502   \u251c\u2500\u2500 efs.tf\\n\u2502   \u2502   \u251c\u2500\u2500 iam.tf\\n\u2502   \u2502   \u251c\u2500\u2500 logs.tf\\n\u2502   \u2502   \u251c\u2500\u2500 security.tf\\n\u2502   \u2502   \u2514\u2500\u2500 providers.tf\\n\u2502   \u2502\\n\\nAs we go in this tutorial, we will be populating individual files.\\n```\\nThe AWS provider that is being used is `version = \\"~> 5.91.0\\"`.\\n\\nFist of, lets populate the `variables.tf`:\\n```terraform\\nvariable \\"aws_region\\" {\\n  description = \\"AWS region where resources will be created\\"\\n  type        = string\\n  default     = \\"YOUR-AWS-REGION\\" \\n}\\n\\nvariable \\"tag_name\\" { # for cost monitoring\\n  description = \\"Name tag for all DevLake resources\\"\\n  type        = string\\n  default     = \\"devlake\\"\\n}\\n```\\n\\n\\n# High level architecture\\nThe goal is to create an AWS ECS cluster running on fargate spot instances, with an RDS mysql database and efs for grafana as a storage layer. We also need an application load balancer on top which we can optionally configure authentication (like OKTA).\\n\\n![Architecture Image](./architecture.png)\\n\\n## Storage\\nLet\'s start with provisioning the necessary storage layer. First of, let\'s create a database and tis security group in the `rds.tf` file. For that we will need the subnets that you want to deploy your database, the vpc id and the database password (adding them to the `variable.tf`):\\n\\n```terraform\\nmodule \\"devlake-rds\\" {\\n  source  = \\"terraform-aws-modules/rds/aws\\"\\n  version = \\"~> 6.10.0\\"\\n\\n  identifier = \\"devlake-db\\"\\n\\n  engine               = \\"mysql\\"\\n  engine_version       = \\"8.0\\"\\n  instance_class       = \\"db.t3.micro\\"\\n  allocated_storage    = 20\\n  major_engine_version = \\"8.0\\"\\n  family               = \\"mysql8.0\\"\\n\\n  manage_master_user_password = false \\n  db_name                     = \\"devlake\\"\\n  username                    = \\"devlake\\"\\n  password                    = var.db_password # todo\\n  port                        = 3306\\n\\n  create_db_subnet_group = true\\n  subnet_ids             = var.private_subnets # todo\\n  vpc_security_group_ids = [aws_security_group.rds.id]\\n\\n  skip_final_snapshot = true\\n\\n  enabled_cloudwatch_logs_exports = [\\"error\\", \\"general\\", \\"slowquery\\"]\\n  create_cloudwatch_log_group     = true\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n\\nresource \\"aws_security_group\\" \\"rds\\" {\\n  name        = \\"devlake-rds\\"\\n  description = \\"Security group for DevLake RDS\\"\\n  vpc_id      = var.vpc_id # todo\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n\\n  ingress {\\n    from_port       = 3306\\n    to_port         = 3306\\n    protocol        = \\"tcp\\"\\n    security_groups = [aws_security_group.ecs_tasks.id]\\n  }\\n}\\n\\n```\\n\\nAnd also let\'s create the EFS for grafana and its own security group in `efs.tf`. Since we know that grafana will also need to access this efs, lets go ahead and also create the access point and mount targets:\\n```terraform\\nresource \\"aws_efs_file_system\\" \\"grafana\\" {\\n  creation_token = \\"grafana-storage\\"\\n  encrypted      = true\\n\\n  lifecycle_policy {\\n    transition_to_ia = \\"AFTER_30_DAYS\\"\\n  }\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n\\nresource \\"aws_efs_access_point\\" \\"grafana\\" {\\n  file_system_id = aws_efs_file_system.grafana.id\\n\\n  posix_user {\\n    gid = 472\\n    uid = 472\\n  }\\n\\n  root_directory {\\n    path = \\"/var/lib/grafana\\" # from devlake documentation\\n    creation_info { # this is necessary to give write permissions\\n      owner_gid   = 472\\n      owner_uid   = 472\\n      permissions = \\"755\\"\\n    }\\n  }\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n\\nresource \\"aws_efs_mount_target\\" \\"grafana\\" {\\n  count           = length(var.private_subnets)\\n  file_system_id  = aws_efs_file_system.grafana.id\\n  subnet_id       = var.private_subnets[count.index]\\n  security_groups = [aws_security_group.efs.id]\\n}\\n\\nresource \\"aws_security_group\\" \\"efs\\" {\\n  name        = \\"grafana-efs\\"\\n  description = \\"Security group for Grafana EFS mount targets\\"\\n  vpc_id      = var.vpc_id\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n\\n  ingress {\\n    from_port       = 2049\\n    to_port         = 2049\\n    protocol        = \\"tcp\\"\\n    security_groups = [aws_security_group.ecs_tasks.id]\\n  }\\n} \\n```\\n\\n## Cluster\\n\\nLet\'s start with creating a cluster in the module\'s `main.tf`:\\n```\\nresource \\"aws_ecs_cluster\\" \\"devlake\\" {\\n  name = \\"devlake-cluster\\"\\n\\n  setting {\\n    name  = \\"containerInsights\\"\\n    value = \\"enabled\\"\\n  }\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n```\\n\\nWe also need to make sure that the services can talk to each other, therefore we need to create a service discovery namespace. This is crucial to allow for proper host resolution (side note, I also tested this with service connect and it did not work):\\n\\n```\\nresource \\"aws_service_discovery_private_dns_namespace\\" \\"devlake\\" {\\n  name        = \\"devlake-ns\\"\\n  description = \\"Private DNS namespace for DevLake services\\"\\n  vpc         = var.vpc_id\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n\\n# one for each service\\nresource \\"aws_service_discovery_service\\" \\"devlake\\" {\\n  name = \\"devlake\\" // \\"config-ui\\" // \\"grafana\\"\\n\\n  dns_config {\\n    namespace_id = aws_service_discovery_private_dns_namespace.devlake.id\\n\\n    dns_records {\\n      ttl  = 10\\n      type = \\"A\\"\\n    }\\n\\n    routing_policy = \\"MULTIVALUE\\"\\n  }\\n\\n  health_check_custom_config {\\n    failure_threshold = 1\\n  }\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n```\\n\\n### Permissions\\nWe also need to create IAM roles with necessary permissions to run our cluster. And a separate task role for the grafana container for efs permissions:\\n```\\nresource \\"aws_iam_role\\" \\"ecs_task_execution_role\\" {\\n  name = \\"devlake-ecs-task-execution-role\\"\\n\\n  assume_role_policy = jsonencode({\\n    Version = \\"2012-10-17\\"\\n    Statement = [\\n      {\\n        Action = \\"sts:AssumeRole\\"\\n        Effect = \\"Allow\\"\\n        Principal = {\\n          Service = \\"ecs-tasks.amazonaws.com\\"\\n        }\\n      }\\n    ]\\n  })\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n\\nresource \\"aws_iam_role_policy_attachment\\" \\"ecs_task_execution_role_policy\\" {\\n  role       = aws_iam_role.ecs_task_execution_role.name\\n  policy_arn = \\"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\\"\\n}\\n\\nresource \\"aws_iam_role_policy_attachment\\" \\"ecs_task_execution_role_policy_cloudwatch\\" {\\n  role       = aws_iam_role.ecs_task_execution_role.name\\n  policy_arn = \\"arn:aws:iam::aws:policy/CloudWatchLogsFullAccess\\"\\n}\\n\\n# IAM Task Role for Grafana container\\nresource \\"aws_iam_role\\" \\"grafana_task_role\\" {\\n  name = \\"devlake-grafana-task-role\\"\\n\\n  assume_role_policy = jsonencode({\\n    Version = \\"2012-10-17\\"\\n    Statement = [\\n      {\\n        Action = \\"sts:AssumeRole\\"\\n        Effect = \\"Allow\\"\\n        Principal = {\\n          Service = \\"ecs-tasks.amazonaws.com\\"\\n        }\\n      }\\n    ]\\n  })\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n\\n# Policy allowing Grafana task to access EFS\\nresource \\"aws_iam_role_policy\\" \\"grafana_efs_access_policy\\" {\\n  name = \\"grafana-efs-access-policy\\"\\n  role = aws_iam_role.grafana_task_role.id\\n\\n  policy = jsonencode({\\n    Version = \\"2012-10-17\\"\\n    Statement = [\\n      {\\n        Effect = \\"Allow\\"\\n        Action = [\\n          \\"elasticfilesystem:ClientMount\\",\\n          \\"elasticfilesystem:ClientWrite\\",\\n          \\"elasticfilesystem:DescribeAccessPoints\\",\\n          \\"elasticfilesystem:DescribeFileSystems\\"\\n        ]\\n        Resource = \\"*\\"\\n      }\\n    ]\\n  })\\n}\\n\\n```\\n\\n\\n### Task definition\\nSince the task definition is more or less identical, I will provide an example for one and just the differences in configuration for others.\\n\\n```\\nresource \\"aws_ecs_task_definition\\" \\"devlake\\" {\\n  family                   = \\"devlake\\"\\n  network_mode             = \\"awsvpc\\"\\n  requires_compatibilities = [\\"FARGATE\\"]\\n  cpu                      = 256\\n  memory                   = 512\\n  execution_role_arn       = aws_iam_role.ecs_task_execution_role.arn\\n\\n  container_definitions = jsonencode([\\n    {\\n      name  = \\"devlake\\"\\n      image = \\"devlake.docker.scarf.sh/apache/devlake:v1.0.1\\"\\n      portMappings = [\\n        {\\n          name          = \\"devlake-port\\" \\n          containerPort = 8080\\n          hostPort      = 8080\\n          protocol      = \\"tcp\\"\\n        }\\n      ]\\n      environment = [\\n        {\\n          name  = \\"DB_URL\\"\\n          value = \\"mysql://devlake:${var.db_password}@${module.devlake-rds.db_instance_endpoint}/devlake?charset=utf8mb4&parseTime=True&loc=UTC\\"\\n        },\\n        {\\n          name  = \\"LOGGING_DIR\\"\\n          value = \\"/app/logs\\"\\n        },\\n        {\\n          name  = \\"TZ\\"\\n          value = \\"UTC\\"\\n        },\\n        {\\n          name  = \\"ENCRYPTION_SECRET\\"\\n          value = var.encryption_secret # your encryption secret\\n        },\\n      ]\\n      logConfiguration = {\\n        logDriver = \\"awslogs\\"\\n        options = {\\n          \\"awslogs-group\\"         = aws_cloudwatch_log_group.devlake.name\\n          \\"awslogs-region\\"        = var.aws_region\\n          \\"awslogs-stream-prefix\\" = \\"devlake\\"\\n        }\\n      }\\n    }\\n  ])\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n```\\n\\nfor config-ui, all is same except for the definition. Take a note on how the endpoints look like, as they are using the namespace we created before:\\n\\n```\\nresource \\"aws_ecs_task_definition\\" \\"config_ui\\" {\\n  family                   = \\"config-ui\\"\\n  ...\\n  container_definitions = jsonencode([\\n    {\\n      name  = \\"config-ui\\"\\n      image = \\"devlake.docker.scarf.sh/apache/devlake-config-ui:latest\\"\\n      portMappings = [\\n        {\\n          name          = \\"config-ui-port\\" \\n          containerPort = 4000\\n          hostPort      = 4000\\n          protocol      = \\"tcp\\"\\n        }\\n      ]\\n      environment = [\\n        {\\n          \\"name\\" : \\"DEVLAKE_ENDPOINT\\",\\n          \\"value\\" : \\"devlake.devlake-ns:8080\\"\\n        },\\n        {\\n          name  = \\"GRAFANA_ENDPOINT\\"\\n          value = \\"grafana.devlake-ns:3000\\"\\n        },\\n        {\\n          name  = \\"TZ\\"\\n          value = \\"UTC\\"\\n        }\\n      ]\\n      logConfiguration = {\\n        logDriver = \\"awslogs\\"\\n        options = {\\n          \\"awslogs-group\\"         = aws_cloudwatch_log_group.config_ui.name\\n          \\"awslogs-region\\"        = var.aws_region\\n          \\"awslogs-stream-prefix\\" = \\"config-ui\\"\\n        }\\n      }\\n    }\\n  ])\\n  ...\\n}\\n```\\n\\nand for grafana, we need to define the task role as well as the volume mount:\\n```\\nresource \\"aws_ecs_task_definition\\" \\"grafana\\" {\\n  family                   = \\"grafana\\"\\n  ...\\n  task_role_arn            = aws_iam_role.grafana_task_role.arn\\n\\n  container_definitions = jsonencode([\\n    {\\n      name  = \\"grafana\\"\\n      image = \\"devlake.docker.scarf.sh/apache/devlake-dashboard:v1.0.1\\"\\n      portMappings = [\\n        {\\n          name          = \\"grafana-port\\" \\n          containerPort = 3000\\n          hostPort      = 3000\\n          protocol      = \\"tcp\\"\\n        }\\n      ]\\n      environment = [\\n        {\\n          name  = \\"GF_SERVER_ROOT_URL\\"\\n          value = \\"https://devlake.${var.domain_name}/grafana\\"\\n        },\\n        {\\n          name  = \\"TZ\\"\\n          value = \\"UTC\\"\\n        },\\n        {\\n          name  = \\"MYSQL_URL\\"\\n          value = module.devlake-rds.db_instance_endpoint\\n        },\\n        {\\n          name  = \\"MYSQL_DATABASE\\"\\n          value = \\"devlake\\"\\n        },\\n        {\\n          name  = \\"MYSQL_USER\\"\\n          value = \\"devlake\\"\\n        },\\n        {\\n          name  = \\"MYSQL_PASSWORD\\"\\n          value = var.db_password\\n        }\\n      ]\\n      mountPoints = [\\n        {\\n          sourceVolume  = \\"grafana-storage\\"\\n          containerPath = \\"/var/lib/grafana\\"\\n          readOnly      = false\\n        }\\n      ]\\n      logConfiguration = {\\n        logDriver = \\"awslogs\\"\\n        options = {\\n          \\"awslogs-group\\"         = aws_cloudwatch_log_group.grafana.name\\n          \\"awslogs-region\\"        = var.aws_region\\n          \\"awslogs-stream-prefix\\" = \\"grafana\\"\\n        }\\n      }\\n    }\\n  ])\\n\\n  volume {\\n    name = \\"grafana-storage\\"\\n    efs_volume_configuration {\\n      file_system_id     = aws_efs_file_system.grafana.id\\n      root_directory     = \\"/\\"\\n      transit_encryption = \\"ENABLED\\"\\n      authorization_config {\\n        access_point_id = aws_efs_access_point.grafana.id\\n        iam             = \\"ENABLED\\"\\n      }\\n    }\\n  }\\n  ...\\n}\\n```\\n\\n### Security \\nTo create the service we also need to create a task security group in `security.tf`. \\n```\\nresource \\"aws_security_group\\" \\"ecs_tasks\\" {\\n  name        = \\"devlake-ecs-tasks\\"\\n  description = \\"Security group for ECS tasks\\"\\n  vpc_id      = var.management_vpc.vpc_id\\n\\n  ingress {\\n    from_port   = 8080\\n    to_port     = 8080\\n    protocol    = \\"tcp\\"\\n    self        = true\\n    description = \\"Allow traffic from other ECS tasks to DevLake API\\"\\n  }\\n\\n  egress {\\n    from_port   = 0\\n    to_port     = 0\\n    protocol    = \\"-1\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n```\\n\\nWe also need to add ingress rules to the RDS and EFS security groups in `rds.tf` and `efs.tf` so that the tasks can reach these services.\\n\\n```\\nresource \\"aws_security_group\\" \\"rds\\" {\\n  ...\\n  ingress {\\n    from_port       = 3306\\n    to_port         = 3306\\n    protocol        = \\"tcp\\"\\n    security_groups = [aws_security_group.ecs_tasks.id]\\n  }\\n  ...\\n}\\n```\\n\\n```\\nresource \\"aws_security_group\\" \\"efs\\" {\\n  ...\\n  ingress {\\n    from_port       = 2049\\n    to_port         = 2049\\n    protocol        = \\"tcp\\"\\n    security_groups = [aws_security_group.ecs_tasks.id]\\n  }\\n  ...\\n} \\n```\\n\\n### Logs\\n\\nTo ensure proper monitoring and debugging capabilities, we\'ll set up CloudWatch log groups for each service. \\n\\n```\\nresource \\"aws_cloudwatch_log_group\\" \\"devlake\\" {\\n  name              = \\"/ecs/devlake\\"\\n  retention_in_days = 30\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n\\nresource \\"aws_cloudwatch_log_group\\" \\"config_ui\\" {\\n  name              = \\"/ecs/config-ui\\"\\n  retention_in_days = 30\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n\\nresource \\"aws_cloudwatch_log_group\\" \\"grafana\\" {\\n  name              = \\"/ecs/grafana\\"\\n  retention_in_days = 30\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n```\\n\\n\\n### Services\\n\\nNow that we have our infrastructure and task definitions set up, let\'s create the ECS services. We\'ll deploy three services: DevLake, Config UI, and Grafana. Each service will use Fargate Spot instances for cost optimization and will be configured with service discovery for internal communication.\\n\\n```\\nresource \\"aws_ecs_service\\" \\"devlake\\" {\\n  name            = \\"devlake\\"\\n  cluster         = aws_ecs_cluster.devlake.id\\n  task_definition = aws_ecs_task_definition.devlake.arn\\n  desired_count   = 1\\n\\n  health_check_grace_period_seconds = 120 # 2 minutes grace period for startup\\n\\n  capacity_provider_strategy {\\n    base              = 1\\n    weight            = 100\\n    capacity_provider = \\"FARGATE_SPOT\\"\\n  }\\n\\n  network_configuration {\\n    subnets         = var.management_vpc.private_subnets\\n    security_groups = [aws_security_group.ecs_tasks.id]\\n  }\\n\\n  # Configure Service Discovery\\n  service_registries {\\n    registry_arn = aws_service_discovery_service.devlake.arn\\n  }\\n\\n  deployment_circuit_breaker {\\n    enable   = true\\n    rollback = true\\n  }\\n\\n  # Configure deployment settings\\n  deployment_maximum_percent         = 100\\n  deployment_minimum_healthy_percent = 0\\n\\n  propagate_tags = \\"SERVICE\\"\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n\\n```\\nTake a note of the deployment setting in devlake service configuration. This is not needed for config ui or grafana, but only for devlake as only one instance od devlake can connect to DB, therefore we need to first kill the instance completely before starting a new one.\\n```\\nresource \\"aws_ecs_service\\" \\"config_ui\\" {\\n  name            = \\"config-ui\\"\\n  task_definition = aws_ecs_task_definition.config_ui.arn\\n  \\n  ...\\n\\n  service_registries {\\n    registry_arn = aws_service_discovery_service.config_ui.arn\\n  }\\n\\n  # Wait for DevLake to be ready\\n  depends_on = [aws_ecs_service.devlake]\\n}\\n\\n```\\n\\n```\\nresource \\"aws_ecs_service\\" \\"grafana\\" {\\n  name            = \\"grafana\\"\\n  task_definition = aws_ecs_task_definition.grafana.arn\\n  \\n  ...\\n\\n  service_registries {\\n    registry_arn = aws_service_discovery_service.grafana.arn\\n  }\\n\\n  # Wait for both DevLake and Config UI to be ready\\n  depends_on = [aws_ecs_service.devlake, aws_ecs_service.config_ui]\\n}\\n```\\n\\nAt this point your containers should be up and running, without any errors regarding DB connection or write permissions to efs. Now we need to setup the load balancer and domain name.\\n\\n## Load balancer\\n\\nLoad balancer helps us route secure traffic to correct containers. We want to have a human readable url, like https://devlake.YOUR-DOMAIN.com to access config ui and https://devlake.YOUR-DOMAIN.com/grafana to access grafana dashboards.\\n\\n\\n### Security\\n\\nFirts we need to create a security group for our alb in `security.tf`:\\n```\\nresource \\"aws_security_group\\" \\"alb\\" {\\n  name        = \\"devlake-alb\\"\\n  description = \\"Security group for DevLake ALB\\"\\n  vpc_id      = var.vpc_id\\n\\n  ingress {\\n    from_port   = 80\\n    to_port     = 80\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n    description = \\"Allow HTTP traffic\\"\\n  }\\n\\n  ingress {\\n    from_port   = 443\\n    to_port     = 443\\n    protocol    = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n    description = \\"Allow HTTPS traffic\\"\\n  }\\n\\n  egress {\\n    from_port   = 0\\n    to_port     = 0\\n    protocol    = \\"-1\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n    description = \\"Allow all outbound traffic\\"\\n  }\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n} \\n```\\nWe also need to allow traffic from alb to the task security groups. This creates a secure path for external traffic to reach our services:\\n\\n```\\nresource \\"aws_security_group\\" \\"ecs_tasks\\" {\\n  ...\\n\\n  ingress {\\n    from_port       = 8080\\n    to_port         = 8080\\n    protocol        = \\"tcp\\"\\n    security_groups = [aws_security_group.alb.id]\\n    description     = \\"Allow traffic from ALB to DevLake API\\"\\n  }\\n\\n  ingress {\\n    from_port       = 4000\\n    to_port         = 4000\\n    protocol        = \\"tcp\\"\\n    security_groups = [aws_security_group.alb.id]\\n    description     = \\"Allow traffic from ALB to DevLake UI\\"\\n  }\\n\\n  ingress {\\n    from_port       = 3000\\n    to_port         = 3000\\n    protocol        = \\"tcp\\"\\n    security_groups = [aws_security_group.alb.id]\\n    description     = \\"Allow traffic from ALB to Grafana UI\\"\\n  }\\n  ...\\n}\\n```\\n\\n### Certificate\\nTo create an alb we also need a valid certificate. The certificate must be in the same region as the alb. This are probably created outside of the devlake module.\\n\\n```\\nresource \\"aws_acm_certificate\\" \\"devlake_cert\\" {\\n  domain_name       = \\"*.YOUR-DOMAIN.COM\\"\\n  validation_method = \\"DNS\\"\\n\\n  lifecycle {\\n    create_before_destroy = true\\n  }\\n\\n  tags = {\\n    Name = \\"devlake\\"\\n  }\\n}\\n\\nresource \\"aws_route53_record\\" \\"cert_validation\\" {\\n  for_each = {\\n    for dvo in aws_acm_certificate.devlake_cert.domain_validation_options : dvo.domain_name => {\\n      name   = dvo.resource_record_name\\n      record = dvo.resource_record_value\\n      type   = dvo.resource_record_type\\n    }\\n  }\\n\\n  allow_overwrite = true\\n  zone_id         = YOUR_DOMAIN_ROUTE53.zone_id\\n  name            = each.value.name\\n  type            = each.value.type\\n  records         = [each.value.record]\\n  ttl             = 60\\n}\\n\\n```\\n\\n### ALB\\nTo create alb, we need two variables - public subnets and certificate created in the previous step. \\n\\nTarget groups are essential components that route traffic to our ECS tasks. We\'ll create two target groups:\\n1. Grafana target group - routes traffic to port 3000 and checks health at `/api/health`\\n2. Config UI target group - routes traffic to port 4000 and checks health at the root path `/`\\n\\nThe health checks ensure that traffic is only routed to healthy containers, maintaining service reliability.\\n\\nListeners define how the ALB routes incoming traffic to our target groups. \\n\\nThe HTTPS listener uses our ACM certificate and routes traffic based on host headers:\\n- `devlake.${var.domain_name}/grafana*` \u2192 Grafana target group\\n- `devlake.${var.domain_name}` \u2192 Config UI target group\\n\\nThis setup ensures secure access to our services while maintaining proper routing based on the requested paths.\\n\\n```\\nmodule \\"devlake_alb\\" {\\n  source  = \\"terraform-aws-modules/alb/aws\\"\\n  version = \\"~> 9.13.0\\"\\n\\n  name = \\"devlake-alb\\"\\n\\n  load_balancer_type = \\"application\\"\\n  internal           = false\\n  security_groups    = [aws_security_group.alb.id]\\n  subnets            = var.public_subnets\\n  vpc_id             = var.vpc_id\\n\\n  listeners = {\\n    https = {\\n      port            = 443\\n      protocol        = \\"HTTPS\\"\\n      ssl_policy      = \\"ELBSecurityPolicy-TLS13-1-2-Res-2021-06\\"\\n      certificate_arn = var.certificate_arn\\n      \\n      # default action\\n      forward = {\\n        target_group_key = \\"devlake-config-ui-tg\\"\\n      }\\n\\n      rules = {\\n        grafana = {\\n          priority = 100\\n          actions = [{\\n            type             = \\"forward\\"\\n            target_group_key = \\"devlake-grafana-tg\\"\\n          }]\\n          conditions = [{\\n            host_header = {\\n              values = [\\"devlake.${var.domain_name}\\"]\\n            }\\n            path_pattern = {\\n              values = [\\"/grafana\\", \\"/grafana/*\\"]\\n            }\\n          }]\\n        }\\n      }\\n    }\\n  }\\n\\n  target_groups = {\\n    \\"devlake-grafana-tg\\" = {\\n      name        = \\"devlake-grafana-tg\\"\\n      protocol    = \\"HTTP\\"\\n      port        = 3000\\n      target_type = \\"ip\\"\\n      health_check = {\\n        enabled             = true\\n        interval            = 30\\n        path                = \\"/api/health\\"\\n        port                = \\"traffic-port\\"\\n        timeout             = 5\\n        healthy_threshold   = 3\\n        unhealthy_threshold = 3\\n        matcher             = \\"200\\"\\n      }\\n      create_attachment = false\\n    },\\n\\n    \\"devlake-config-ui-tg\\" = {\\n      name        = \\"devlake-config-ui-tg\\"\\n      protocol    = \\"HTTP\\"\\n      port        = 4000\\n      target_type = \\"ip\\"\\n      health_check = {\\n        enabled             = true\\n        interval            = 30\\n        path                = \\"/\\"\\n        port                = \\"traffic-port\\"\\n        timeout             = 5\\n        healthy_threshold   = 3\\n        unhealthy_threshold = 3\\n        matcher             = \\"200\\"\\n      }\\n      create_attachment = false\\n    }\\n  }\\n\\n  tags = {\\n    Name = var.tag_name\\n  }\\n}\\n\\n```\\n\\n### DNS Records\\n\\nFinally, to use a human readable url, we need to create DNS records in our route53 that points to our alb. For simplicity\'s sake, I am referencing directly the alb module\'s outputs, in reality you will probably need to ooutput these from the devlake module itself: \\n\\n```\\nresource \\"aws_route53_record\\" \\"devlake\\" {\\n  zone_id = YOUR_DOMAIN_ROUTE53.zone_id\\n  name    = \\"devlake.YOUR-DOMAIN.com\\"\\n  type    = \\"A\\"\\n\\n  alias {\\n    name                   = module.devlake_alb.dns_name\\n    zone_id                = module.devlake_alb.zone_id\\n    evaluate_target_health = true\\n  }\\n}\\n\\n```\\n\\nAnd thats it, you should be able to access devlake on https://devlake.YOUR-DOMAIN.com!\\n\\n# Cost breakdown\\n If you want to monitor your costs, first of you need to provide tags with all your resources and second you need to [activate the tag on aws](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/activating-tags.html) so it is registered. \\n\\n![Cost breakdown Image](./cost.png)\\n\\nThe daily cost is stable on aroun `$1.51` a day, which gives about `$45` a month. The cost does not include the route53 domain though."},{"id":"Quick-Start-Guide-Setup-Your-First-Engineering-Metrics-Dashboard-in-5-Minutes","metadata":{"permalink":"/zh/blog/Quick-Start-Guide-Setup-Your-First-Engineering-Metrics-Dashboard-in-5-Minutes","editUrl":"https://github.com/apache/incubator-devlake-website/edit/main/blog/2024-04-24-Quick-Start-Guide-Setup-Your-first-Engineering-Metrics-in-5-minutes/index.md","source":"@site/blog/2024-04-24-Quick-Start-Guide-Setup-Your-first-Engineering-Metrics-in-5-minutes/index.md","title":"Quick Start Guide: Setup Your First Engineering Metrics Dashboard in 5 Minutes","description":"In the marathon of software development, teams can be overwhelmed by the complexity of tasks and the pressure to perform. The real challenge lies in understanding which efforts lead to success and which need refinement. That\'s the power of metrics. They transform endless streams of data into actionable insights. Apache DevLake provides the means to harness these insights, offering a metrics dashboard that aligns daily tasks with strategic goals. This guide is your first step towards a more informed and intentional approach to development. Let\u2019s embark on this journey to make every commit, issue, and pull request an opportunity for improvement.","date":"2024-04-24T00:00:00.000Z","formattedDate":"2024\u5e744\u670824\u65e5","tags":[{"label":"DevLake","permalink":"/zh/blog/tags/dev-lake"},{"label":"Engineering Metrics","permalink":"/zh/blog/tags/engineering-metrics"},{"label":"Grafana","permalink":"/zh/blog/tags/grafana"}],"readingTime":2.37,"hasTruncateMarker":true,"authors":[{"name":"Joshua Poddoku","title":"Apache DevLake Committer","url":"https://github.com/JoshuaPoddoku","imageURL":"https://github.com/JoshuaPoddoku.png","key":"Joshua"}],"frontMatter":{"slug":"Quick-Start-Guide-Setup-Your-First-Engineering-Metrics-Dashboard-in-5-Minutes","title":"Quick Start Guide: Setup Your First Engineering Metrics Dashboard in 5 Minutes","authors":"Joshua","tags":["DevLake","Engineering Metrics","Grafana"]},"prevItem":{"title":"Setting up Devlake in AWS ECS cluster using Terraform","permalink":"/zh/blog/setting-up-devlake-in-aws-ecs-cluster-using-terraform"},"nextItem":{"title":"DevLake Playground: How to explore your data","permalink":"/zh/blog/DevLake-Playground-How-to-explore-your-data"}},"content":"In the marathon of software development, teams can be overwhelmed by the complexity of tasks and the pressure to perform. The real challenge lies in understanding which efforts lead to success and which need refinement. That\'s the power of metrics. They transform endless streams of data into actionable insights. Apache DevLake provides the means to harness these insights, offering a metrics dashboard that aligns daily tasks with strategic goals. This guide is your first step towards a more informed and intentional approach to development. Let\u2019s embark on this journey to make every commit, issue, and pull request an opportunity for improvement.\\n\\n\x3c!--truncate--\x3e\\n\\n![Cover Image](./Cover-Image.png)\\n\\n## Getting Started\\nBefore setting up your first Engineering Metrics Dashboard with Apache DevLake, ensure you have the following **pre-requisites** ready:\\n- **DevLake Version:** This feature is available from 1.0.0-beta3 or higher [[Installation Guide](https://devlake.apache.org/docs/GettingStarted)]\\n- **Repository:** Your code should be on GitHub, GitLab, BitBucket, or Azure DevOps. This requirement is just for the initial setup; you can connect other tools later.\\n\\n## Set Up The Workflow\\nAfter installation, connect to your first repository by opening the Config UI or `localhost:4000/onboard` and you will start the onboarding session as shown in the screenshot below\\n![Start Onboarding Session](./Onboarding-Session.png)\\n\\n### Step-1: Create Project\\nIn this step, you\'ll define a project that aims to enhance your software development process within your organization or team. Remember, the true value lies in the goals behind the metrics rather than the metrics themselves. Focus on setting clear, strategic objectives to ensure that the metrics you track are meaningful and directly contribute to your broader organizational goals.\\n![Step-1: Create Project](./Step-1.png)\\n\\n### Step-2: Configure Connection\\n- Initiate by creating a Personal Access Token (PAT) on your chosen platform\u2014be it [GitHub](https://devlake.apache.org/docs/Configuration/GitHub/#github-personal-access-tokensrecommended), [GitLab](https://devlake.apache.org/docs/Configuration/GitLab/#personal-access-token), [BitBucket](https://devlake.apache.org/docs/Configuration/BitBucket/#username-and-app-password), or [Azure DevOps](https://devlake.apache.org/docs/Configuration/AzureDevOps/#tokenhttps://devlake.apache.org/docs/Configuration/AzureDevOps#token)\\n- This token empowers DevLake to securely interact with the specific repository you\'re setting up for metric analysis.\\n![Step-2: Configure Connection](./Step-2.png)\\n\\n### Step-3: Add Data Scope\\nAdding your data scope is a crucial step where you select the specific repository you want to monitor. Once selected, the dashboard aggregates data pertaining to issues, deployments, pull requests (PRs), workflow runs, GitHub actions, and more, giving you a comprehensive view of your operations. \\nTo streamline the onboarding experience and swiftly deliver your initial metrics, the system is configured to collect only the **last 14 days of data** by default. This ensures a rapid setup so you can start analyzing your metrics almost immediately. However, this setting is flexible\u2014 you can adjust the data collection window at any time to suit your project\u2019s needs and objectives.\\n\\n![Step-3: Add Data Scope](./Step-3-1.png)\\n\\n![Step-3: Processing Data](./Step-3-2.png)\\n\\n## Metrics Dashboard Overview\\nThis metrics dashboard provides a clear visualization of the development processes by tracking various key performance indicators from GitHub. The dashboard covers:\\n- **User Requirements (Issues)**: Monitors new issues and how they are resolved over time, providing insights into the demands and responsiveness of the team.\\n- **Issue Resolution**: Shows the number of closed issues and the average time taken for issue resolution, offering a measure of efficiency and agility in problem-solving.\\n- **Pull Requests**: Details the number of new pull requests, an indicator of collaboration, alongside the merge time, indicating the pace of code integration.\\n- **Workflow Runs**: Highlights the total number of workflow runs and their success rates, illustrating the robustness of the CI/CD pipeline.\\n- **Queue and Processing Times**: Analyzes the time taken from issue opening to closure and pull request opening to merge, shedding light on the throughput and process optimization.\\n\\nBy consolidating these metrics, the dashboard serves as a crucial tool for teams to evaluate their development lifecycle and make data-driven improvements.\\n\\n![Grafana Dashboard](./Grafana-Dashboard.png)\\n\\n## Next Steps\\nCongratulations on setting up your first engineering metrics dashboard using GitHub and Grafana in just a few minutes! As an open-source tool, DevLake [supports various data sources and plugins](https://devlake.apache.org/docs/Overview/SupportedDataSources/) while ensuring that your data remains securely under your control. Learn more about how DevLake can help you by checking the [DORA guide](https://devlake.apache.org/docs/DORA/), how to [customize dashboards](https://devlake.apache.org/docs/Configuration/Dashboards/GrafanaUserGuide#customizing-a-dashboard), and how to [customize data model](https://devlake.apache.org/docs/DataModels/DevLakeDomainLayerSchema/#how-to-customize-data-models).\\n\\nIf you haven\'t already, star [DevLake GitHub repository](https://github.com/apache/incubator-devlake) to follow the updates and feel free to post your queries on [slack](https://join.slack.com/t/devlake-io/shared_invite/zt-2ox842kuu-_6x3Lwdj88YpzKhMRpgnMg) - our vibrant community includes engineering leaders, users, experts, and contributors from diverse backgrounds around the world.\\n\\nWatch this tutorial on [YouTube](https://www.youtube.com/watch?v=DPp6PIUDE1k)"},{"id":"DevLake-Playground-How-to-explore-your-data","metadata":{"permalink":"/zh/blog/DevLake-Playground-How-to-explore-your-data","editUrl":"https://github.com/apache/incubator-devlake-website/edit/main/blog/2024-03-15-DevLake-Playground-How-to-explore-your-data/index.md","source":"@site/blog/2024-03-15-DevLake-Playground-How-to-explore-your-data/index.md","title":"DevLake Playground: How to explore your data","description":"DevLake is a compelling offering.","date":"2024-03-15T00:00:00.000Z","formattedDate":"2024\u5e743\u670815\u65e5","tags":[{"label":"devlake","permalink":"/zh/blog/tags/devlake"},{"label":"playground","permalink":"/zh/blog/tags/playground"},{"label":"python","permalink":"/zh/blog/tags/python"},{"label":"process mining","permalink":"/zh/blog/tags/process-mining"}],"readingTime":2.64,"hasTruncateMarker":true,"authors":[{"name":"Jochum B\xf6rger","title":"Apache DevLake Contributor","url":"https://github.com/jochumb","imageURL":"https://github.com/jochumb.png","key":"Jochum"},{"name":"Lennart Tange","title":"Apache DevLake Contributor","url":"https://github.com/lenntt","imageURL":"https://github.com/lenntt.png","key":"Lennart"}],"frontMatter":{"slug":"DevLake-Playground-How-to-explore-your-data","title":"DevLake Playground: How to explore your data","authors":["Jochum","Lennart"],"tags":["devlake","playground","python","process mining"]},"prevItem":{"title":"Quick Start Guide: Setup Your First Engineering Metrics Dashboard in 5 Minutes","permalink":"/zh/blog/Quick-Start-Guide-Setup-Your-First-Engineering-Metrics-Dashboard-in-5-Minutes"},"nextItem":{"title":"Compatibility of Apache DevLake with PostgreSQL","permalink":"/zh/blog/compatibility-of-apache-devLake-with-postgreSQL"}},"content":"DevLake is a compelling offering.\\nIt collects and normalizes data from many of our favorite development tools and visualizes it using Grafana dashboards.\\nLike the sleuths we are, we feel the urge to look beyond the dashboard overviews and find the golden nuggets buried deep within the data.\\nSo, we\'d like to introduce the DevLake Playground, a place where you can unleash the power of Python on your data.\\n\\n\x3c!--truncate--\x3e\\n\\nIn the DevLake Playground, we can explore the data using Jupyter Notebooks.\\nThere are some predefined notebooks and you can write your own. \\nA Jupyter notebook combines Python code and documentation, which you can easily customize to your needs with some tweaks.\\nThe benefits of these Jupyter notebooks as opposed to Grafana are:\\n- Grafana is limited to SQL queries for gathering and transforming data, and visualizations for data tables.\\n- Python (code) offers more flexibility in transforming the data and can easily provide feedback on intermediate steps.\\n- The playground also supports more visualization types; for example (see the first use case below,) when the data is structured as a graph, we can visualize it with Graphviz.\\n\\n\\n## Use cases\\n\\n### Analyzing the development process through JIRA statuses\\n\\nThe [DevLake Domain model](https://devlake.apache.org/docs/DataModels/DevLakeDomainLayerSchema#schema-diagram) exposes the changes of issues of our issue tracker, including status changes.\\nIf we use that to visualize how issues really flow, we get a rudimentary (automated) value stream map. \\nWe can use this to identify bottlenecks in our process or flaws in our process design.\\nThis is inspired by [this blog post](https://xebia.com/blog/insights-from-your-jira-data-to-help-improve-your-team/):\\n\\n![process graph](./processgraph.png)\\n\\nFor example, in the chart above, we see that it takes on average 15 days for Stories to go from \\"Ready\\" to \\"In Progress\\". \\nAnd, it happened 476x within the selected time frame.\\n\\nAnd now that we have this data in our playground, we can easily change how we represent it. \\nIf we focus on the most common status transitions, we can visualize the distribution of durations in a box plot out of the same data:\\n\\n![box plot](./boxplot.png)\\n\\nThis functionality is made available through a [predefined notebook](https://github.com/apache/incubator-devlake-playground/blob/main/notebooks/process_analysis.ipynb), so you can easily run it with your own data.\\n\\n\\n### Explore data across different domains\\n\\nLet\'s say we have a hypothesis: \\"Defect fixes are more quickly merged than new functionality.\\" \\nBefore building a dashboard, we want to determine whether the data quality is good enough and whether we can test this hypothesis.\\nWith [pandas](https://pandas.pydata.org/), we can quickly join different tables from the [data model](https://devlake.apache.org/docs/DataModels/DevLakeDomainLayerSchema#schema-diagram). \\nWith the following code, we were able to get a preliminary view:\\n\\n```python\\nimport pandas as pd\\n\\nfrom playground.db_engine import create_db_engine\\n\\n# the default notebook\\nDB_URL = \\"mysql://merico:merico@127.0.0.1:3306/lake\\"\\nengine = create_db_engine(DB_URL)\\n\\n# read tables from database\\ndf_pr_issues = pd.read_sql(\\"select * from pull_request_issues\\", engine)\\ndf_prs = pd.read_sql(\\"select * from pull_requests\\", engine)\\ndf_issues = pd.read_sql(\\"select * from issues\\", engine)\\n\\n# join pull requests and issues based on rows in pull_request_issues\\ndf = pd.merge(df_pr_issues, df_prs, left_on=\\"pull_request_id\\", right_on=\\"id\\", suffixes=(\'_pr_issues\', \'_prs\'))\\ndf = pd.merge(df, df_issues, left_on=\\"issue_id\\", right_on=\\"id\\", suffixes=(\'_prs\', \'_issues\'))\\n\\n# set data types correctly\\ndf[\'created_date_issues\'] = pd.to_datetime(df[\'created_date_issues\'])\\ndf[\'resolution_date\'] = pd.to_datetime(df[\'resolution_date\'])\\ndf[\'created_date_prs\'] = pd.to_datetime(df[\'created_date_prs\'])\\ndf[\'merged_date\'] = pd.to_datetime(df[\'merged_date\'])\\n# calculate lead times\\ndf[\'issue_lead_time\'] = df[\'resolution_date\'] - df[\'created_date_issues\']\\ndf[\'pr_lead_time\'] = df[\'merged_date\'] - df[\'created_date_prs\']\\n# drop unnecessary columns\\ndf = df[[\'type_issues\', \'title_issues\', \'issue_lead_time\', \'title_prs\', \'pr_lead_time\']]\\n\\n# group lead times by issue_type, add count\\ndf_grouped = df.groupby(\'type_issues\').agg({\\n    \'title_issues\': \'count\', \\n    \'issue_lead_time\': [\'mean\', \'median\', \'std\'], \\n    \'pr_lead_time\': [\'mean\', \'median\', \'std\']\\n})\\ndf.rename(columns={\'title_issues\': \'issue_count\'}, inplace=True)\\n\\ndisplay(df_grouped)\\n```\\n\\nIf we run this example on the Devlake GitHub issues and pull requests (up to March 2024), we get the following output:\\n\\n|             | title_issues | issue_lead_time            |                         |                            | pr_lead_time              |                        |                            |\\n| ----------- | ------------ | -------------------------- | ----------------------- | -------------------------- | ------------------------- | ---------------------- | -------------------------- |\\n|             | count        | mean                       | median                  | std                        | mean                      | median                 | std                        |\\n| type_issues |              |                            |                         |                            |                           |                        |                            |\\n|             | 34           | 66 days 14:53:28.709677419 | 42 days 05:59:04        | 67 days 14:35:49.143870568 | 3 days 11:42:34.857142857 | 0 days 12:54:43.500000 | 12 days 17:24:54.878541108 |\\n| BUG         | 141          | 10 days 07:27:20.572463768 | 1 days 22:42:39         | 20 days 14:51:35.075965706 | 1 days 16:51:51.529411764 | 0 days 01:04:54        | 10 days 10:20:54.566875184 |\\n| INCIDENT    | 2            | 0 days 00:50:49            | 0 days 00:50:49         | 0 days 00:59:50.688234865  | 0 days 00:06:39           | 0 days 00:06:39        | 0 days 00:00:42.426406871  |\\n| REQUIREMENT | 40           | 37 days 02:50:22.500000    | 16 days 03:56:45.500000 | 60 days 02:39:39.606995949 | 9 days 11:13:27.270270270 | 2 days 00:14:04        | 22 days 12:32:27.522638402 |\\n\\n\\n## Getting started\\n\\nWe hope you are as excited as we are.\\nWe look forward to you joining our community to get your feedback and contributions.\\n\\nWant to get started? \\nHave a look at the [playground repository](https://github.com/apache/incubator-devlake-playground)."},{"id":"compatibility-of-apache-devLake-with-postgreSQL","metadata":{"permalink":"/zh/blog/compatibility-of-apache-devLake-with-postgreSQL","editUrl":"https://github.com/apache/incubator-devlake-website/edit/main/blog/2022-06-23-compatibility-of-apache-devLake-with-postgreSQL/index.md","source":"@site/blog/2022-06-23-compatibility-of-apache-devLake-with-postgreSQL/index.md","title":"Compatibility of Apache DevLake with PostgreSQL","description":"Apache DevLake is a dev data platform that can collect and integrate data from different dev tools including Jira, Github, GitLab and Jenkins.","date":"2022-06-23T00:00:00.000Z","formattedDate":"2022\u5e746\u670823\u65e5","tags":[{"label":"devlake","permalink":"/zh/blog/tags/devlake"},{"label":"database","permalink":"/zh/blog/tags/database"},{"label":"postgresql","permalink":"/zh/blog/tags/postgresql"}],"readingTime":2.5833333333333335,"hasTruncateMarker":false,"authors":[{"name":"ZhangLiang","title":"Apache DevLake PPMC","url":"https://github.com/mindlesscloud","imageURL":"https://avatars.githubusercontent.com/u/8455907?s=400&v=4","key":"ZhangLiang"}],"frontMatter":{"slug":"compatibility-of-apache-devLake-with-postgreSQL","title":"Compatibility of Apache DevLake with PostgreSQL","authors":"ZhangLiang","tags":["devlake","database","postgresql"]},"prevItem":{"title":"DevLake Playground: How to explore your data","permalink":"/zh/blog/DevLake-Playground-How-to-explore-your-data"},"nextItem":{"title":"How DevLake is Up and Running","permalink":"/zh/blog/how-DevLake-is-up-and-running"}},"content":"Apache DevLake is a dev data platform that can collect and integrate data from different dev tools including Jira, Github, GitLab and Jenkins.\\n\\nThis blog will not aim at a comprehensive summary of the compatibility of database but a record of issues for future reference.\\n\\n## 1.Different  Data Types\\n### PostgreSQL does not have a uint type\\n```sql=\\ntype JenkinsBuild struct {\\n common.NoPKModel\\n JobName           string  `gorm:\\"primaryKey;type:varchar(255)\\"`\\n Duration          float64 // build time\\n DisplayName       string  // \\"#7\\"\\n EstimatedDuration float64\\n Number            int64 `gorm:\\"primaryKey;type:INT(10) UNSIGNED NOT NULL\\"`\\n Result            string\\n Timestamp         int64     // start time\\n StartTime         time.Time // convered by timestamp\\n CommitSha         string\\n}\\n```\\n\\nIn `JenkinsBuild.Number`, the`gorm`struct tag used `UNSIGNED`, which will lead to the failure to create table and should be removed.\\n\\n![](https://i.imgur.com/N7E9Vwd.png)\\n\\n\\n### MySQL does not have a bool data type\\n\\nFor a field defined as bool type in model, gorm will map it to MySQL\'s TINYINT data type, which can be queried directly with 0 or 1 in SQL, but PostgreSQL has a bool type, so gorm will map it to the BOOL type. If 0 or 1 is still used in SQL to query, there will be a report of error.\\n\\nHere is an example(only relevant fields are shown in the example). The lookup statement works in MySQL, but will lead to an error in PostgreSQL.\\n\\n```sql=\\ntype GitlabMergeRequestNote struct {\\n MergeRequestId  int    `gorm:\\"index\\"`\\n System          bool \\n}\\n \\ndb.Where(\\"merge_request_id = ? AND `system` = 0\\", gitlabMr.GitlabId).\\n```\\n\\nAfter changing the sentence as it follows, an error will still be reported. The reason will be shown in the part about backticks.\\n\\n```sql=\\ndb.Where(\\"merge_request_id = ? AND `system` = ?\\", gitlabMr.GitlabId, false)\\n```\\n\\n## 2.Different Behaviors\\n\\n### Bulk insertion\\nWhen `ON CONFLIT UPDATE ALL` was used to achieve bulk insertion, and if there are multiple records with the same primary key, it will report errors in PostgreSQL but not in MySQL.\\n![](https://i.imgur.com/zaExAUG.png)\\n\\n![](https://i.imgur.com/BpZY8dN.png)\\n\\n### Inconsistent definition of model with schema\\nFor example, in the model definition, `GithubPullRequest.AuthorId` is of the int type, but this field in the database is of VARCHAR type. When inserting data, MySQL will accept it, but ProstgresSQL will report an error.\\n```sql=\\ntype GithubPullRequest struct {\\n GithubId        int    `gorm:\\"primaryKey\\"`\\n RepoId          int    `gorm:\\"index\\"`\\n Number          int    `gorm:\\"index\\"` \\n State           string `gorm:\\"type:varchar(255)\\"`\\n Title           string `gorm:\\"type:varchar(255)\\"`\\n GithubCreatedAt time.Time\\n GithubUpdatedAt time.Time `gorm:\\"index\\"`\\n ClosedAt        *time.Time\\n // In order to get the following fields, we need to collect PRs individually from GitHub\\n Additions      int\\n Deletions      int\\n Comments       int\\n Commits        int\\n ReviewComments int\\n Merged         bool\\n MergedAt       *time.Time\\n Body           string\\n Type           string `gorm:\\"type:varchar(255)\\"`\\n Component      string `gorm:\\"type:varchar(255)\\"`\\n MergeCommitSha string `gorm:\\"type:varchar(40)\\"`\\n HeadRef        string `gorm:\\"type:varchar(255)\\"`\\n BaseRef        string `gorm:\\"type:varchar(255)\\"`\\n BaseCommitSha  string `gorm:\\"type:varchar(255)\\"`\\n HeadCommitSha  string `gorm:\\"type:varchar(255)\\"`\\n Url            string `gorm:\\"type:varchar(255)\\"`\\n AuthorName     string `gorm:\\"type:varchar(100)\\"`\\n AuthorId       int\\n common.NoPKModel\\n}\\n```\\n![](https://i.imgur.com/onxGG8d.png)\\n\\n## 3.MySQL-Specific Functions\\n\\nWe used the `GROUP_CONCAT`function in a complex query. Although there are similar functions in PostgreSQL, the function names are different and the usage is slightly different.\\n\\n```sql=\\ncursor2, err := db.Table(\\"pull_requests pr1\\").\\n  Joins(\\"left join pull_requests pr2 on pr1.parent_pr_id = pr2.id\\").Group(\\"pr1.parent_pr_id, pr2.created_date\\").Where(\\"pr1.parent_pr_id != \'\'\\").\\n  Joins(\\"left join repos on pr2.base_repo_id = repos.id\\").\\n  Order(\\"pr2.created_date ASC\\").\\n  Select(`pr2.key as parent_pr_key, pr1.parent_pr_id as parent_pr_id, GROUP_CONCAT(pr1.base_ref order by pr1.base_ref ASC) as cherrypick_base_branches, \\n   GROUP_CONCAT(pr1.key order by pr1.base_ref ASC) as cherrypick_pr_keys, repos.name as repo_name, \\n   concat(repos.url, \'/pull/\', pr2.key) as parent_pr_url`).Rows()\\n```\\n\\nSolution:\\nWe finally decided to use two steps to achieve the `GROUP_CONCAT` function. First we used the simplest SQL query to get multiple pieces of the sorted data, and then used the code to group them.\\n\\nAfter modification:\\n```sql=\\n    cursor2, err := db.Raw(\\n  `\\n   SELECT pr2.pull_request_key                 AS parent_pr_key,\\n          pr1.parent_pr_id                     AS parent_pr_id,\\n          pr1.base_ref                         AS cherrypick_base_branch,\\n          pr1.pull_request_key                 AS cherrypick_pr_key,\\n          repos.NAME                           AS repo_name,\\n          Concat(repos.url, \'/pull/\', pr2.pull_request_key) AS parent_pr_url,\\n        pr2.created_date\\n   FROM   pull_requests pr1\\n          LEFT JOIN pull_requests pr2\\n                 ON pr1.parent_pr_id = pr2.id\\n          LEFT JOIN repos\\n                 ON pr2.base_repo_id = repos.id\\n   WHERE  pr1.parent_pr_id != \'\'\\n   ORDER  BY pr1.parent_pr_id,\\n             pr2.created_date,\\n       pr1.base_ref ASC\\n   `).Rows()\\n```\\n\\n## 4.Different Grammar\\n### Backticks\\nWe used backticks in some SQL statements to protect field names from conflicting with MySQL reserved words, which can lead to errors in PostgreSQL. To solve this problem we revisited our code, modified all field names that conflict with reserved words, and removed the backticks in the SQL statement. In the example just mentioned:\\n\\n```sql=\\ndb.Where(\\"merge_request_id = ? AND `system` = ?\\", gitlabMr.GitlabId, false)\\n```\\n\\nSolution:\\nWe changed `system` to `is_system` to avoid the usage of backticks.\\n```sql=\\ndb.Where(\\"merge_request_id = ? AND is_system = ?\\", gitlabMr.GitlabId, false)\\n```\\n\\n### Non-standard delete statement\\nThere were delete statements as followed in our code, which are legal in MySQL but will report an error in PostgreSQL.\\n```sql=\\nerr := db.Exec(`\\n DELETE ic\\n FROM jira_issue_commits ic\\n LEFT JOIN jira_board_issues bi ON (bi.source_id = ic.source_id AND bi.issue_id = ic.issue_id)\\n WHERE ic.source_id = ? AND bi.board_id = ?\\n `, sourceId, boardId).Error\\n```"},{"id":"how-DevLake-is-up-and-running","metadata":{"permalink":"/zh/blog/how-DevLake-is-up-and-running","editUrl":"https://github.com/apache/incubator-devlake-website/edit/main/blog/2022-06-17-How DevLake is up and running/index.md","source":"@site/blog/2022-06-17-How DevLake is up and running/index.md","title":"How DevLake is Up and Running","description":"Apache DevLake is an integration tool with the DevOps data collection functionality, which presents a different stage of data to development teams via Grafana. which also can leverage teams to improve the development process with a data-driven model.","date":"2022-06-17T00:00:00.000Z","formattedDate":"2022\u5e746\u670817\u65e5","tags":[{"label":"devlake","permalink":"/zh/blog/tags/devlake"},{"label":"apache","permalink":"/zh/blog/tags/apache"}],"readingTime":3.466666666666667,"hasTruncateMarker":true,"authors":[{"name":"Warren Chen","title":"Apache DevLake PPMC","url":"https://github.com/warren830","imageURL":"https://github.com/warren830.png","key":"warren"}],"frontMatter":{"slug":"how-DevLake-is-up-and-running","title":"How DevLake is Up and Running","authors":"warren","tags":["devlake","apache"]},"prevItem":{"title":"Compatibility of Apache DevLake with PostgreSQL","permalink":"/zh/blog/compatibility-of-apache-devLake-with-postgreSQL"},"nextItem":{"title":"Apache Incubator Welcomes DevLake, A Dev-Data Platform Serving Developers","permalink":"/zh/blog/apache-welcomes-devlake"}},"content":"[Apache DevLake](https://github.com/apache/incubator-devlake) is an integration tool with the DevOps data collection functionality, which presents a different stage of data to development teams via Grafana. which also can leverage teams to improve the development process with a data-driven model.\\n\\n### Apache DevLack Architecture Overview\\n- The left side of the following screenshot is an [integrative DevOps data plugin](https://devlake.apache.org/docs/Overview/SupportedDataSources/), the existing plugins include Github, GitLab, JIRA, Jenkins, Tapd, Feishu, and the most featured analysis engine in the Simayi platform.\\n- The main framework in the middle of the following screenshot, completes data collection, expansion, and conversion to the domain layer by running subtasks in the plugins. The user can trigger the tasks by config-UI or all API.\\n- RMDBS currently supports Mysql and PostgresSQL, more databases will be supported in the future.\\n- Grafana can generate different types of needed data by using SQL.\\n\\n![Generated](Aspose.Words.093a76ac-457b-4498-a472-7dbea580bca9.001.png)\\n\\n> Then let\u2019s move on to how to start running DevLake.\\n\\n\x3c!--truncate--\x3e\\n\\n### Start the system\\nBefore the Golang program runs, it will automatically call the init() method in the package. We need to focus on the loading of the services package. The following code has detailed comments:\\n```go\\nfunc init() {\\nvar err error\\n// get initial config information\\ncfg = config.GetConfig()\\n// get Database\\ndb, err = runner.NewGormDb(cfg, logger.Global.Nested(\\"db\\"))\\n// configure time zone\\nlocation := cron.WithLocation(time.UTC)\\n// create scheduled task manager\\ncronManager = cron.New(location)\\nif err != nil {\\npanic(err)\\n}\\n// initialize the data migration\\nmigration.Init(db)\\n// register the framework\'s data migration scripts\\nmigrationscripts.RegisterAll()\\n// load plugin, loads all .so files in the folder cfg.GetString(\\"PLUGIN_DIR\\")\uff0cin th LoadPlugins method()\uff0cspecifically, LoadPlugins stores the pluginName:PluginMeta key-value pair into core.plugins by calling runner.\\nerr = runner.LoadPlugins(\\ncfg.GetString(\\"PLUGIN_DIR\\"),\\ncfg,\\nlogger.Global.Nested(\\"plugin\\"),\\ndb,\\n)\\nif err != nil {\\npanic(err)\\n}\\n// run data migration scripts to complete the initializztion work of tables in the databse framework layer.\\nerr = migration.Execute(context.Background())\\nif err != nil {\\npanic(err)\\n}\\n\\n// call service init\\npipelineServiceInit()\\n}\\n\\n```\\n### The execution principle of DevLake\\n**The running process of the pipeline**\\nBefore we go through the pipeline process, we need to know the [Blueprint](https://devlake.apache.org/docs/Overview/KeyConcepts#blueprints) first.\\n\\nBlueprint is a timed task that contains all the subtasks and plans that need to be executed. Each execution record of Blueprint is a historical run, AKA Pipeline. Which presents a trigger for DevLack to complete one or more data collection transformation tasks through one or more plugins.\\n\\n![Generated](Aspose.Words.093a76ac-457b-4498-a472-7dbea580bca9.002.png)\\n\\nThe following is the pipeline running flow chart.\\n\\n![Generated](Aspose.Words.093a76ac-457b-4498-a472-7dbea580bca9.003.png)\\n\\nA pipeline contains a two-dimensional array of tasks, mainly to ensure that a series of tasks are executed in a preset order. Like the following screenshot if the plugin of Stage 3 needs to rely on the other plugin to prepare the data(eg: the operation of refdiff needs to rely on gitextractor and Github, for more information on data sources and plugins, please refer to the [documentation](https://devlake.apache.org/docs/Overview/SupportedDataSources/), then when Stage 3 starts to execute, it needs to ensure that its dependencies have been executed in Stage 1 and Stage 2.\\n\\n![Generated](Aspose.Words.093a76ac-457b-4498-a472-7dbea580bca9.004.png)\\n\\n**Task running process**\\n\\nThe plugin tasks in stage1, stage2, and stage3 are executed in parallel:\\n\\n![Generated](Aspose.Words.093a76ac-457b-4498-a472-7dbea580bca9.005.png)\\n\\n**The next step is to execute the subtasks in the plugin sequentially.**\\n\\n![Generated](Aspose.Words.093a76ac-457b-4498-a472-7dbea580bca9.006.png)\\n\\n1. The work before the RunTask is to prepare the params for the RunTask method to call, like logger, db, context and etc.\\n2. The main method of RunTask is mainly to update the tasks in the Database, at the same time prepare to run the options of the plugins task.\\n3. RunpluginTask will obtain the corresponding  [PluginMeta](#pm) through core.Getplugin(pluginName), then obtains the [PluginTask](#pt) via PluginMeta, and then executes RunPluginSubTasks.\\n\\n**The running process of each plugin subtask(the relevant interface and func will be explained in the next section)**\\n\\n![Generated](Aspose.Words.093a76ac-457b-4498-a472-7dbea580bca9.007.png)\\n\\n1. Get all available subtasks subtaskMeta of all th4e plugins by calling SubTaskMetas().\\n2. Use options[\u2018task\u2019] and subtaskMeta to form a set of subtasks to be executed subtaskMetas.\\n3. Calculate how many subtasks in total\\n4. Build taskCtx via helper.NewDefaultTaskContext.\\n5. Build taskData via call pluginTask.PrepareTaskData.\\n6. Iterate over all subtasks in subtaskMetas.\\n    1. Get subtaskCtx of subtask via call taskCtx.SubTaskContext(subtaskMeta.Name).\\n    2. Run subtaskMeta.EntryPoint(subtaskCtx)\\n## Important interfaces in DevLake\\n1. <a id=\\"pm\\">PluginMeta</a>: Contains the two basic methods of plugins, which all plugins need to implement. And stored in core.plugins when the system starts. And obtained through core.GetPlugin when executing plugin tasks.\\n\\n```go\\ntype PluginMeta interface {\\n   Description() string\\n   //PkgPath information will be lost when compiled as plugin(.so), this func will return that info\\n   RootPkgPath() string\\n}\\n\\n```\\n\\n2. <a id=\\"pt\\">PluginTask</a>: It can be obtained by PluginMeta, after the plugin implemented this method, Framework can run the subtask directly, instead of letting the plugin itself run it, the biggest benefit of this is that the subtasks of the plugin are easier to implement, and we can more easily leverage(such as adding logs, etc.) during the operation of the plugin.\\n\\n```go\\ntype PluginTask interface {\\n   // return all available subtasks, framework will run them for you in order\\n   SubTaskMetas() []SubTaskMeta\\n   // based on task context and user input options, return data that shared among all subtasks\\n   PrepareTaskData(taskCtx TaskContext, options map[string]interface{}) (interface{}, error)\\n}\\n```\\n\\n3. Every plugin has a <a id=\\"td\\">taskData</a>, which contains configuration options, apiClient, and other properties of plugins.(like the github has repo information)\\n4. <a id=\\"stm\\">SubTaskMeta</a>:: the meta data of a subtask, every subtask will define a SubTaskMeta.\\n\\n```go\\nvar CollectMeetingTopUserItemMeta = core.SubTaskMeta{\\n   Name: \\"collectMeetingTopUserItem\\",\\n   EntryPoint: CollectMeetingTopUserItem,\\n   EnabledByDefault: true,\\n   Description: \\"Collect top user meeting data from Feishu api\\",\\n}\\n```\\n\\n5. <a id=\\"ec\\">ExecContext</a>: defines all resources needed to execute (sub)tasks.\\n6. <a id=\\"stc\\">SubTaskContext</a>: defines all resources need to execute subtask(including ExecContext).\\n7. <a id=\\"tc\\">TaskContext</a>: defines all resources need to execute tasks(including ExecContext). The difference with SubTaskContext is the TaskContext() method in SubTaskContext can retire TaskContext, but SubTaskContext(subtask string) method in TaskContext can return SubTaskContext, which means the subtask belongs to the plugin task, so we use the different contexts to distinguish this.\\n8. <a id=\\"step\\">SubTaskEntryPoint</a>: all the subtasks in the plugin have to implement this function so that they can be coordinated and arranged by the framework layer.\\n\\n## Further Plan\\nThis blog introduced the basics of the DevLack framework and how it starts and runs, there are 3 more contexts api\\\\_collector, api\\\\_extractor, and data\\\\_convertor will be explained in the next blog."},{"id":"apache-welcomes-devlake","metadata":{"permalink":"/zh/blog/apache-welcomes-devlake","editUrl":"https://github.com/apache/incubator-devlake-website/edit/main/blog/2022-05-18-apache-welcomes-devLake/index.md","source":"@site/blog/2022-05-18-apache-welcomes-devLake/index.md","title":"Apache Incubator Welcomes DevLake, A Dev-Data Platform Serving Developers","description":"We are excited to share today that the Apache Software Foundation (ASF) voted to make DevLake an officially supported project of the Apache Incubator.","date":"2022-05-18T00:00:00.000Z","formattedDate":"2022\u5e745\u670818\u65e5","tags":[{"label":"Devlake","permalink":"/zh/blog/tags/devlake"},{"label":"Apache","permalink":"/zh/blog/tags/apache"}],"readingTime":1.64,"hasTruncateMarker":true,"authors":[{"name":"Maxim Wheatley","title":"Apache DevLake PPMC","url":"https://github.com/MaximDub","imageURL":"https://github.com/MaximDub.png","key":"maxim"}],"frontMatter":{"slug":"apache-welcomes-devlake","title":"Apache Incubator Welcomes DevLake, A Dev-Data Platform Serving Developers","authors":"maxim","tags":["Devlake","Apache"]},"prevItem":{"title":"How DevLake is Up and Running","permalink":"/zh/blog/how-DevLake-is-up-and-running"}},"content":"We are excited to share today that the Apache Software Foundation (ASF) voted to make DevLake an officially supported project of the Apache Incubator.\\n\\n\\n\\n### What is DevLake?\\n\\nLaunched in December of 2021, [Apache DevLake](https://github.com/apache/incubator-devlake) is an open-source dev data platform that ingests, analyzes, and visualizes the fragmented data in developer tools. \\n\\nSoftware development is complex, requiring many tools and processes, and as a result creates a storm of data scattered across tools in many formats. This makes it difficult to organize, query, and make sense of. We built Apache DevLake, to make it easy to make sense of this rich data and to translate it into actionable insights.\\n\\n\\n\x3c!--truncate--\x3e\\n\\n\\n### Apache DevLake\'s key features:\\n\\n- DevOps data collection across software development lifecycle (SDLC) to connect data islands\\n- Standardized data models with out-of-the-box metrics and customizable dashboards\\n- Flexible plugin system for user-defined data integration and transformation\\n\\nBelow is the architecture of Apache DevLake: \\n![architecture](0.11-architecture-diagram.jpg)\\n\\n\\n### Why join Apache Incubator?\\n\\nFirst and foremost, we firmly believe in Apache\'s principle of \\"Community over Code\\" and [\\"The Apache Way\\"](https://www.apache.org/theapacheway/index.html). We look forward to building a vibrant, inclusive, and diverse community under the guidance of our mentors through the incubator journey.\\n\\nWe also feel that the Apache community has one of the strongest ecosystems when it comes to data-oriented open-source projects. \\n\\nLast and not least, the support and enthusiasm of the Apache community and mentors made it clear to us that this would be a place where we can truly evolve and nurture Apache DevLake.\\n\\nIn the months leading up to DevLake\'s acceptance, our mentors provided guidance and suggestions instrumental to making this important moment a reality. We want to take this opportunity to thank and acknowledge them: \\n\\n\\n\\n- **[Willem Ning Jiang](https://github.com/WillemJiang)**: Apache DevLake Champion; ASF Member & Board Director\\n- **[Liang Zhang](https://github.com/terrymanu)**: Founder & CEO of SphereEx; ASF Member, Founder & PMC Chair of Apache ShardingSphere\\n- **[Lidong Dai](https://github.com/dailidong)**: ASF Member; Apache DolphinScheduler PMC Chair\\n- **[Sijie Guo](https://github.com/sijie)**: ASF Member, PMC member of Apache Pulsar; Founder & CEO of StreamNative\\n\\n- **[Felix Cheung](https://github.com/felixcheung)**: ASF Member, PMC on Spark, Superset, Yunikorn, Zeppelin, Pinot, and Incubator. SVP of Engineering at SafeGraph.\\n\\n- **[Jean-Baptiste Onofr\xe9](https://github.com/jbonofre)**: ASF Member, Karaf PMC Chair, PMC on ActiveMQ, Archiva, Aries, Beam, Brooklyn, Camel, Carbondata, Felix, Incubator, and [many more](http://people.apache.org/committer-index.html).\\n\\n\\n\\n### Apache DevLake Future Roadmap\\n\\n- Enhance system scalability and performance in large-scale data scenarios.\\n- Integrate more data sources and tools (JIRA, GitHub, GitLab, and Jenkins are already supported.)\\n- Enable support for OLAP databases, providing users with more choices.\\n- Provide more scenario-specific, out-of-the-box dashboards and templates reflecting best practices and well-known methodologies to improve usability.\\n\\n\\n\\n### Join us! \\n\\nWe invite developers and those passionate about data-driven engineering to \'dive into the lake\' with us, and welcome contributions of all kinds. \\nJoin us on Slack and at our weekly open source community meetups\ud83e\udd73\\n\\n**Apache DevLake (Incubating) Links:**\\n\\n- GitHub:  https://github.com/apache/incubator-devlake\\n- Official Website: https://devlake.apache.org/\\n- Slack:  https://join.slack.com/t/devlake-io/shared_invite/zt-2ox842kuu-_6x3Lwdj88YpzKhMRpgnMg\\n- Podling Website\uff1ahttps://incubator.apache.org/projects/devlake.html"}]}')}}]);